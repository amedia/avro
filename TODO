 go test -coverpkg ./... -coverprofile /tmp/prof.out ./... && go tool cover -html /tmp/prof.out

Tests still to do:

	union with more than two types that includes nil
	false boolean
	out of bounds cases
	fuzzing
	zero length byte read
	zero length string read
	recursive types

	multiple inData, outData instances per generated test package.

	many error cases:
		see coverage.

Features still to do:

- String/MarshalText/UnmarshalText methods on enums
- support normal Go types without AvroRecord methods.
	- what do we do about interface{} values?
		We're filling in the whole thing from a known schema, so we could
		fill in the holes with pieces from the writer's schema,
		and record the entire schema at the top level:

		package avro

		type Dynamic struct {
			schema string
			value interface{}
		}

		func (d Dynamic) Schema() string

- support for logical types.
	- we'd like to use time.Time but that won't look correct
	when converted to JSON
	-
- registry support
	- https://docs.confluent.io/current/schema-registry/serializer-formatter.html
- use nil for maps and slices instead of pointers
- optimisation:
	- cache type to unmarshaler mapping
	- sync.Pool
	initially, maybe just have a map of (type, schema) -> program
- Codec
- Encoder
- Decoder
- think about non-record types at the top level of a schema
- better namespace support
	- command-line flag to control mapping of namespaces to directories
- JSON marshaling and unmarshaling
	- need to decide whether to use standard Avro JSON format
	for all cases, even though it doesn't correspond with standard
	representation.
	- alternative JSON representation:
		for unions, if all cases are distinguishable by looking at the
		top level JSON type of the non-union JSON representation
		(e.g. number vs string vs bool vs object vs null vs array)
		then encode directly in the value.
	- could also allow representation of timestamps as RFC3339 strings
- make Unmarshal fail if there are extra bytes in the data.

// Marshal encodes the given value as
// a message using the Avro binary encoding.
//
// See https://avro.apache.org/docs/current/spec.html#binary_encoding
//
// Currently, x must be a type that was generated
// by the avro-generate-go command.
func Marshal(x interface{}) (_ []byte, marshalErr error)

// Unmarshal unmarshals the given Avro-encoded binary
// data, written with the given schema, into v, which should
// be a pointer to a struct type generated by avro-generate-go.
// TODO provide a way of unmarshaling single-object-encoded
// data along with a way of retrieving external schemas.
func Unmarshal(data []byte, x interface{}, writerSchema string) error

// NewEncoder returns a new Encoder instance that encodes
// a stream of messages into r using the given schema.
// All messages encoded must have the same Avro schema.
//
// By default, it encodes the Avro object container file (OCF) format,
// writing the schema header when the first value is encoded.
func NewEncoder(w io.Writer, schema string) *Encoder

// Encode writes a message to the encoder.
// All messages must have the same Avro schema.
func (enc *Encoder) Encode(x interface{}) error

// OmitHeader specifies that no file header should be written,
// just the encoded values themselves without the schema.
func (enc *Encoder) OmitHeader()

// UseJSON causes gthe header and messages to be encoded to the stream
// in Avro JSON encoding instead of binary.
// See https://avro.apache.org/docs/1.8.2/spec.html#json_encoding
func (enc *Encoder) UseJSON()

type Decoder struct {
	readerSchema schema.AvroType
	progs        map[reflect.Type]*program
}

// NewDecoder returns a new Decoder instance that
// decodes a stream of messages from r.
// By default it decodes the Avro object container
// file (OCF) format, setting the schema by reading it
// from the start of the file.
// See https://avro.apache.org/docs/1.8.2/spec.html#Object+Container+Files
func NewDecoder(r io.Reader) *Decoder

// SetSchema sets the schema used to decode the
// message stream. If this is called before any messages
// are read, the decoder will not read the OCF
// header.
func (dec *Decoder) SetSchema(schema string) error

// SetJSON causes messages to be decoded from the
// Avro JSON encoding instead of binary.
// See https://avro.apache.org/docs/1.8.2/spec.html#json_encoding
func (dec *Decoder) UseJSON()

// Buffered returns a reader of the data remaining in the Decoder's
// buffer. The reader is valid until the next call to Decode.
func (dec *Decoder) Buffered() io.Reader

// Decode decodes the next value into x, which should
// be a pointer to an Avro-compatible type (see
// Unmarshal for further details).
func (dec *Decoder) Decode(x interface{}) error


// Codec can be used to decode
// messages that have been individually encoded according
// different schemas. Unlike Decoder and Encoder it does not
// act on a stream of messages, but instead assumes that
// the messages have been obtained from somewhere else
// (for example from a Kafka topic).
type Codec struct {
}

// NewCodec returns a new Codec
// that uses g to determine the schema of each
// message that's marshaled or unmarshaled.
func NewCodec(g SchemaGetter) *Codec

// Marshal marshals the given value and returns the resulting
// message.
func (u *Codec) Marshal(x interface{}) ([]byte, error)

// Unmarshal unmarshals the given data into x.
// It needs the context argument because it might end up
// fetching schema data over the network via the Codec's
// associated SchemaGetter.
func (u *Codec) Unmarshal(ctx context.Context, data []byte, x interface{}) error

// SchemaGetter is used by an Unmarshaler to find information
// about the schema used to encode a message.
// One notable implementation is avroregistry.Registry.
type SchemaGetter interface {
	// SchemaID returns the schema ID of the message
	// and the bare message without schema information.
	// A schema ID is specific to the SchemaGetter instance - within
	// a given SchemaGetter instance (only), a given schema ID
	// must always correspond to the same schema.
	//
	// If the message isn't valid, SchemaID should return (0, nil).
	SchemaID(msg []byte) (int64, []byte)

	// AppendWithSchemaID appends the message encoded along with the
	// given schema ID to the given buffer.
	AppendWithSchemaID(buf []byte, msg []byte, id int64) []byte

	// SchemaForID returns the schema for the given ID.
	SchemaForID(ctx context.Context, id int64) (string, error)
}

----------------------------------------

cache pool:

type schemaHash [sha256.HasLen]byte
type progHash struct {
	reader, writer schemaHash
}

map from go type to avro type

maybe we should have the caches inside the Codec, Decoder or Encoder types
so that they can be garbage collected, because they can get arbitrarily large.

	var avroTypes sync.Map		// map[schemaHash] compiler.AvroType
	var avroProgs sync.Map		// map[progHash] *vm.Program
	var typeSchemas sync.Map	// map[reflect.Type] schemaHash

when unmarshaling:
	id, data := split(msg)
	if we've got a program for (id, unmarshalType), use that
	look up or add schema for type to typeSchemas
	look up schema hash for id:
	if found:
		look up the schema in avroTypes
	else
		call SchemaForID
		hash it
		add to avroTypes
	make program with two schemas and type


--------------------------------

logical types:

decimal variable-length:

{
  "type": "bytes",
  "logicalType": "decimal",
  "precision": 4,
  "scale": 2
}

decimal fixed-length:

{
  "type": "fixed",
  "name": "somename",
  "logicalType": "decimal",
  "precision": 4,
  "scale": 2,
  "size": 10
}

uuid:
{
	"type": "string"
	"logicalType": "uuid"
}

date:
{
	"type": "int"
	"logicalType": "data"
}

millisecond time of day (time after midnight):
{
	"type": "int"
	"logicalType": "time-millis"
}

microsecond time of day (time after midnight):
{
	"type": "long"
	"logicalType": "time-micros"
}


millisecond timestamp (from unix epoch)
{
	"type": "int"
	"logicalType": "timestamp-millis"
}

microsecond timestamp (from unix epoch)
{
	"type": "int"
	"logicalType": "timestamp-micros"
}

duration:
A duration logical type annotates Avro fixed type of size 12, which stores three little-endian unsigned integers that represent durations at different granularities of time. The first stores a number in months, the second stores a number in days, and the third stores a number in milliseconds.
{
	"type": "fixed"
	"size": 12
	"name": "duration"
	"logicalType": "duration"
}

the problem with the duration logical type is that it must be different everywhere,
which makes it not very useful for interoperability between different Avro schemas.
Perhaps define a standard avro.Duration type and ignore any definition name?

Could support another logical type, say duration-nanos
to encode Go-style durations. Use a long.

Also, what about timestamp-nanos, perhaps as a 12 byte fixed type
(8 bytes of seconds; 4 bytes of nanoseconds).

Of the above types,