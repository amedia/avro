 go test -coverpkg ./... -coverprofile /tmp/prof.out ./... && go tool cover -html /tmp/prof.out

Tests still to do:

	union with more than two types that includes nil
	false boolean
	out of bounds cases
	fuzzing
	zero length byte read
	zero length string read
	recursive types

	multiple inData, outData instances per generated test package.

	many error cases:
		see coverage.

Features still to do:

- String/MarshalText/UnmarshalText methods on enums
- support normal Go types without AvroRecord methods.
	- what do we do about interface{} values?
		We're filling in the whole thing from a known schema, so we could
		fill in the holes with pieces from the writer's schema,
		and record the entire schema at the top level:

		package avro

		type Dynamic struct {
			schema string
			value interface{}
		}

		func (d Dynamic) Schema() string

- support for logical types.
	- we'd like to use time.Time but that won't look correct
	when converted to JSON
	-
- registry support
	- https://docs.confluent.io/current/schema-registry/serializer-formatter.html
- use nil for maps and slices instead of pointers
- optimisation:
	- cache type to unmarshaler mapping
	- sync.Pool
	initially, maybe just have a map of (type, schema) -> program
- Codec
- Encoder
- Decoder
- think about non-record types at the top level of a schema
- better namespace support
	- command-line flag to control mapping of namespaces to directories
- JSON marshaling and unmarshaling
	- need to decide whether to use standard Avro JSON format
	for all cases, even though it doesn't correspond with standard
	representation.
	- alternative JSON representation:
		for unions, if all cases are distinguishable by looking at the
		top level JSON type of the non-union JSON representation
		(e.g. number vs string vs bool vs object vs null vs array)
		then encode directly in the value.
	- could also allow representation of timestamps as RFC3339 strings
- make Unmarshal fail if there are extra bytes in the data.

// NewEncoder returns a new Encoder instance that encodes
// a stream of messages into r using the given schema.
// All messages encoded must have the same Avro schema.
//
// By default, it encodes the Avro object container file (OCF) format,
// writing the schema header when the first value is encoded.
func NewStreamEncoder(w io.Writer, schema string) *StreamEncoder

// Encode writes a message to the encoder.
// All messages must have the same Avro schema.
func (enc *StreamEncoder) Encode(x interface{}) error

// OmitHeader specifies that no file header should be written,
// just the encoded values themselves without the schema.
func (enc *StreamEncoder) OmitHeader()

// UseJSON causes gthe header and messages to be encoded to the stream
// in Avro JSON encoding instead of binary.
// See https://avro.apache.org/docs/1.8.2/spec.html#json_encoding
func (enc *StreamEncoder) UseJSON()

type StreamDecoder struct {
	readerSchema schema.AvroType
	progs        map[reflect.Type]*program
}

// NewStreamDecoder returns a new StreamDecoder instance that
// decodes a stream of messages from r.
// By default it decodes the Avro object container
// file (OCF) format, setting the schema by reading it
// from the start of the file.
// See https://avro.apache.org/docs/1.8.2/spec.html#Object+Container+Files
func NewStreamDecoder(r io.Reader) *StreamDecoder

// SetSchema sets the schema used to decode the
// message stream. If this is called before any messages
// are read, the decoder will not read the OCF
// header.
func (dec *StreamDecoder) SetSchema(schema string) error

// SetJSON causes messages to be decoded from the
// Avro JSON encoding instead of binary.
// See https://avro.apache.org/docs/1.8.2/spec.html#json_encoding
func (dec *StreamDecoder) UseJSON()

// Buffered returns a reader of the data remaining in the Decoder's
// buffer. The reader is valid until the next call to Decode.
func (dec *StreamDecoder) Buffered() io.Reader

// Decode decodes the next value into x, which should
// be a pointer to an Avro-compatible type (see
// Unmarshal for further details).
func (dec *StreamDecoder) Decode(x interface{}) error

type DecodingRegistry interface {
	// DecodeSchemaID returns the schema ID header of the message
	// and the bare message without schema information.
	// A schema ID is specific to the DecodingRegistry instance - within
	// a given DecodingRegistry instance (only), a given schema ID
	// must always correspond to the same schema.
	//
	// If the message isn't valid, DecodeSchemaID should return (0, nil).
	DecodeSchemaID(msg []byte) (int64, []byte)

	// SchemaForID returns the schema for the given ID.
	SchemaForID(ctx context.Context, id int64) (string, error)
}

type SingleEncoder struct {

}

type EncodingRegistry interface {
	// AppendSchemaID appends the given schema ID header to buf
	// and returns the resulting slice.
	AppendSchemaID(buf []byte, id int64) []byte

	// IDForSchema returns an ID for the given schema.
	// If the schema wasn't found, it returns ErrSchemaNotFound.
	IDForSchema(ctx context.Context, schema string) (int64, error)
}


// NewSingleEncoder returns a SingleEncoder instance that encodes single
// messages along with their schema identifier.
func NewSingleEncoder(r EncodingRegistry) *SingleEncoder

// Marshal returns x marshaled as using the Avro binary encoding,
// along with an identifier that records the type that it was encoded
// with.
func (enc *SingleEncoder) Marshal(ctx context.Context, x interface{}) ([]byte, error) {
	xv := reflect.ValueOf(x)
	avroType, err := avroTypeOf(xv.Type())
	if err != nil {
		return nil, err
	}
	id, err := enc.registry.IDForSchema(ctx, avroType.String())
	if err != nil {
		return nil, err
	}
	buf := make([]byte, 0, 100)
	buf = enc.registry.AppendSchemaID(buf, id)
	return marshalAppend(buf, xv, avroType)
}

type SingleDecoder struct {
}

func NewSingleDecoder(r DecodingRegistry) *SingleDecoder

// Unmarshal unmarshals the given message into x. The body
// of the message is unmarshaled as with the Unmarshal function.
//
// It needs the context argument because it might end up
// fetching schema data over the network via the Codec's
// associated SchemaGetter.
func (c *Codec) Unmarshal(ctx context.Context, data []byte, x interface{}) (*Type, error) {

---------------------------------------------------------
logical types:

decimal variable-length:

{
  "type": "bytes",
  "logicalType": "decimal",
  "precision": 4,
  "scale": 2
}

decimal fixed-length:

{
  "type": "fixed",
  "name": "somename",
  "logicalType": "decimal",
  "precision": 4,
  "scale": 2,
  "size": 10
}

uuid:
{
	"type": "string"
	"logicalType": "uuid"
}

date:
{
	"type": "int"
	"logicalType": "data"
}

millisecond time of day (time after midnight):
{
	"type": "int"
	"logicalType": "time-millis"
}

microsecond time of day (time after midnight):
{
	"type": "long"
	"logicalType": "time-micros"
}


millisecond timestamp (from unix epoch)
{
	"type": "int"
	"logicalType": "timestamp-millis"
}

microsecond timestamp (from unix epoch)
{
	"type": "int"
	"logicalType": "timestamp-micros"
}

duration:
A duration logical type annotates Avro fixed type of size 12, which stores three little-endian unsigned integers that represent durations at different granularities of time. The first stores a number in months, the second stores a number in days, and the third stores a number in milliseconds.
{
	"type": "fixed"
	"size": 12
	"name": "duration"
	"logicalType": "duration"
}

the problem with the duration logical type is that it must be different everywhere,
which makes it not very useful for interoperability between different Avro schemas.
Perhaps define a standard avro.Duration type and ignore any definition name?

Could support another logical type, say duration-nanos
to encode Go-style durations. Use a long.

Also, what about timestamp-nanos, perhaps as a 12 byte fixed type
(8 bytes of seconds; 4 bytes of nanoseconds).

Of the above types,


---------------------------------------------------------

// CompatMode defines a compatiblity mode used for checking Avro
// type compatibility.
type CompatMode int

const (
	Backward CompatMode = 1<<iota
	Forward
	Transitive

	BackwardTransitive = Backward | Transitive
	ForwardTransitive = Forward | Transitive
	Full = Backward | Forward
	FullTransitive = Full | Transitive
)

// Compatible checks whether t is compatible with all the previous schemas
// with respect to the given compatibility mode. The previousSchemas
// slice is in oldest-to-newest order.
//
// If mode doesn't include Transitive, only the latest schema is checked.
func (t *Type) Compatible(mode CompatMode, previousSchemas ...*Type) error

// Subsumes reports whether all the information in a value of Avro type t2
// can be encoded using type t1 without loss.
func (t1 *Type) Subsumes(t2 *Type) bool
