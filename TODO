 go test -coverpkg ./... -coverprofile /tmp/prof.out ./... && go tool cover -html /tmp/prof.out

Tests still to do:

	out of bounds cases
	fuzzing
	zero length byte read
	zero length string read
	recursive types

	multiple inData, outData instances per generated test package.

	many error cases:
		see coverage.

Features still to do:

- registry support
	- https://docs.confluent.io/current/schema-registry/serializer-formatter.html
- use nil for maps and slices instead of pointers
- make Unmarshal fail if there are extra bytes in the data.
- support for logical types.
	- timestamp
	- we'd like to use time.Time but that won't look correct
	when converted to JSON
- String/MarshalText/UnmarshalText methods on enums

- optimisation:
	- benchmarks
- better namespace support
	- command-line flag to control mapping of namespaces to directories
- StreamEncoder
- StreamDecoder
- think about non-record types at the top level of a schema
- support normal Go types with interface{} fields.
		We're filling in the whole thing from a known schema, so we could
		fill in the holes with pieces from the writer's schema,
		and record the entire schema at the top level:

		package avro

		type Dynamic struct {
			schema string
			value interface{}
		}

		func (d Dynamic) Schema() string
- JSON marshaling and unmarshaling
	- need to decide whether to use standard Avro JSON format
	for all cases, even though it doesn't correspond with standard
	representation.
	- alternative JSON representation:
		for unions, if all cases are distinguishable by looking at the
		top level JSON type of the non-union JSON representation
		(e.g. number vs string vs bool vs object vs null vs array)
		then encode directly in the value.
	- could also allow representation of timestamps as RFC3339 strings

// NewEncoder returns a new Encoder instance that encodes
// a stream of messages into r using the given schema.
// All messages encoded must have the same Avro schema.
//
// By default, it encodes the Avro object container file (OCF) format,
// writing the schema header when the first value is encoded.
func NewStreamEncoder(w io.Writer, schema string) *StreamEncoder

// Encode writes a message to the encoder.
// All messages must have the same Avro schema.
func (enc *StreamEncoder) Encode(x interface{}) error

// OmitHeader specifies that no file header should be written,
// just the encoded values themselves without the schema.
func (enc *StreamEncoder) OmitHeader()

// UseJSON causes gthe header and messages to be encoded to the stream
// in Avro JSON encoding instead of binary.
// See https://avro.apache.org/docs/1.8.2/spec.html#json_encoding
func (enc *StreamEncoder) UseJSON()

type StreamDecoder struct {
	readerSchema schema.AvroType
	progs        map[reflect.Type]*program
}

// NewStreamDecoder returns a new StreamDecoder instance that
// decodes a stream of messages from r.
// By default it decodes the Avro object container
// file (OCF) format, setting the schema by reading it
// from the start of the file.
// See https://avro.apache.org/docs/1.8.2/spec.html#Object+Container+Files
func NewStreamDecoder(r io.Reader) *StreamDecoder

// SetSchema sets the schema used to decode the
// message stream. If this is called before any messages
// are read, the decoder will not read the OCF
// header.
func (dec *StreamDecoder) SetSchema(schema string) error

// SetJSON causes messages to be decoded from the
// Avro JSON encoding instead of binary.
// See https://avro.apache.org/docs/1.8.2/spec.html#json_encoding
func (dec *StreamDecoder) UseJSON()

// Buffered returns a reader of the data remaining in the Decoder's
// buffer. The reader is valid until the next call to Decode.
func (dec *StreamDecoder) Buffered() io.Reader

// Decode decodes the next value into x, which should
// be a pointer to an Avro-compatible type (see
// Unmarshal for further details).
func (dec *StreamDecoder) Decode(x interface{}) error

---------------------------------------------------------
logical types:

decimal variable-length:

{
  "type": "bytes",
  "logicalType": "decimal",
  "precision": 4,
  "scale": 2
}

decimal fixed-length:

{
  "type": "fixed",
  "name": "somename",
  "logicalType": "decimal",
  "precision": 4,
  "scale": 2,
  "size": 10
}

uuid:
{
	"type": "string"
	"logicalType": "uuid"
}

date:
{
	"type": "int"
	"logicalType": "data"
}

millisecond time of day (time after midnight):
{
	"type": "int"
	"logicalType": "time-millis"
}

microsecond time of day (time after midnight):
{
	"type": "long"
	"logicalType": "time-micros"
}


millisecond timestamp (from unix epoch)
{
	"type": "int"
	"logicalType": "timestamp-millis"
}

microsecond timestamp (from unix epoch)
{
	"type": "int"
	"logicalType": "timestamp-micros"
}

duration:
A duration logical type annotates Avro fixed type of size 12, which stores three little-endian unsigned integers that represent durations at different granularities of time. The first stores a number in months, the second stores a number in days, and the third stores a number in milliseconds.
{
	"type": "fixed"
	"size": 12
	"name": "duration"
	"logicalType": "duration"
}

the problem with the duration logical type is that it must be different everywhere,
which makes it not very useful for interoperability between different Avro schemas.
Perhaps define a standard avro.Duration type and ignore any definition name?

Could support another logical type, say duration-nanos
to encode Go-style durations. Use a long.

Also, what about timestamp-nanos, perhaps as a 12 byte fixed type
(8 bytes of seconds; 4 bytes of nanoseconds).


---------------------------------------------------------

// CompatMode defines a compatiblity mode used for checking Avro
// type compatibility.
type CompatMode int

const (
	Backward CompatMode = 1<<iota
	Forward
	Transitive

	BackwardTransitive = Backward | Transitive
	ForwardTransitive = Forward | Transitive
	Full = Backward | Forward
	FullTransitive = Full | Transitive
)

// Compatible checks whether t is compatible with all the previous schemas
// with respect to the given compatibility mode. The previousSchemas
// slice is in oldest-to-newest order.
//
// If mode doesn't include Transitive, only the latest schema is checked.
func (t *Type) Compatible(mode CompatMode, previousSchemas ...*Type) error

// Subsumes reports whether all the information in a value of Avro type t2
// can be encoded using type t1 without loss.
func (t1 *Type) Subsumes(t2 *Type) bool
